{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "\n",
    "\n",
    "# Tune Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
    "\n",
    "# Gridsearch Hyperparameters\n",
    "\n",
    "In the guided project, you learned how to use sklearn's GridsearchCV and keras-tuner library to tune the hyperparamters of a neural network model. For your module project you'll continue using these two libraries however we are going to make things a little more interesting for you. \n",
    "\n",
    "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
    "\n",
    "\n",
    "\n",
    "**Don't forgot to switch to GPU on Colab!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# native python libraries imports \n",
    "import math\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# sklearn imports \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# keras imports \n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "# required for compatibility between sklearn and keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quickdraw10():\n",
    "    \"\"\"\n",
    "    Fill out this doc string, and comment the code, for practice in writing the kind of code that will get you hired. \n",
    "    \"\"\"\n",
    "    \n",
    "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
    "    \n",
    "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
    "\n",
    "    data = np.load(path_to_zip)\n",
    "    \n",
    "    # normalize your image data\n",
    "    max_pixel_value = 255\n",
    "    X = data['arr_0']/max_pixel_value\n",
    "    Y = data['arr_1']\n",
    "        \n",
    "    return train_test_split(X, Y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_quickdraw10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 784)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Experiment 1\n",
    "\n",
    "## Tune Hyperperameters using Enhanced GridsearchCV \n",
    "\n",
    "We are going to use GridsearchCV again to tune a deep learning model however we are going to add some additional functionality to our gridsearch. Specifically, we are going to automate away the generation of how many nodes to use in a layer and how many layers to use in a model! \n",
    "\n",
    "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment. \n",
    "\n",
    "\n",
    "### Objective \n",
    "\n",
    "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. Up until now, we've been manually selecting the number of layers and layer nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USXjs7Hk71Hy"
   },
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
    "    \"\"\"\"\n",
    "    Returns a complied keras model \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_layers: int \n",
    "        number of hidden layers in model \n",
    "        To be clear, this excludes the input and output layer.\n",
    "        \n",
    "    first_layer_nodes: int\n",
    "        Number of nodes in the first hidden layer \n",
    "\n",
    "    last_layer_nodes: int\n",
    "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
    "        \n",
    "     act_funct: string \n",
    "         Name of activation function to use in hidden layers (this excludes the output layler)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model: keras object \n",
    "    \"\"\"\n",
    "    \n",
    "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
    "        \"\"\"\n",
    "        Generates and returns the number of nodes in each hidden layer. \n",
    "        To be clear, this excludes the input and output layer. \n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        Number of nodes in each layer is linearly incremented. \n",
    "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers: int\n",
    "            Number of hidden layers\n",
    "            This values should be 2 or greater \n",
    "\n",
    "        first_layer_nodes: int\n",
    "\n",
    "        last_layer_nodes: int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        layers: list of ints\n",
    "            Contains number of nodes for each layer \n",
    "        \"\"\"\n",
    "\n",
    "        # throws an error if n_layers is less than 2 \n",
    "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
    "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
    "        # when set to True number of nodes are decreased for subsequent layers \n",
    "        if negative_node_incrementation:\n",
    "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
    "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
    "            \n",
    "        # when set to False number of nodes are increased for subsequent layers\n",
    "        else:\n",
    "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
    "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
    "\n",
    "        nodes = first_layer_nodes\n",
    "\n",
    "        for i in range(1, n_layers+1):\n",
    "\n",
    "            layers.append(math.ceil(nodes))\n",
    "\n",
    "            # increment nodes for next layer \n",
    "            nodes = nodes + nodes_increment\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
    "    \n",
    "    for i in range(1, n_layers):\n",
    "        if i==1:\n",
    "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
    "        else:\n",
    "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
    "            \n",
    "            \n",
    "    # output layer \n",
    "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
    "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer='adam', # adam is a good default optimizer \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # do not include model.fit() inside the create_model function\n",
    "    # KerasClassifier is expecting a complied model \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore create_model\n",
    "\n",
    "Let's build a few different models in order to understand how the above code works in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "\n",
    "Use `create_model` to build a model. \n",
    "\n",
    "- Set `n_layers = 10` \n",
    "- Set `first_layer_nodes = 500`\n",
    "- Set `last_layer_nodes = 100`\n",
    "- Set `act_funct = \"relu\"`\n",
    "- Make sure that `negative_node_incrementation = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dcf5c585f07629a03086cf57ba53615",
     "grade": false,
     "grade_id": "cell-86d63e89a21223de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use create_model to create a model \n",
    "\n",
    "# YOUR CODE HERE\n",
    "model = create_model(n_layers=10, first_layer_nodes=500, last_layer_nodes=100, act_funct =\"relu\", negative_node_incrementation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 456)               228456    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 412)               188284    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 367)               151571    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 323)               118864    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 278)               90072     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 234)               65286     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 189)               44415     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 145)               27550     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1460      \n",
      "=================================================================\n",
      "Total params: 1,308,458\n",
      "Trainable params: 1,308,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run model.summary() and make sure that you understand the model architecture that you just built \n",
    "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "\n",
    "Use `create_model` to build a model. \n",
    "\n",
    "- Set `n_layers = 10` \n",
    "- Set `first_layer_nodes = 500`\n",
    "- Set `last_layer_nodes = 100`\n",
    "- Set `act_funct = \"relu\"`\n",
    "- Make sure that `negative_node_incrementation = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0722533c325d699f4842e874e43720e",
     "grade": false,
     "grade_id": "cell-99d563a291231a7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use create_model to create a model \n",
    "\n",
    "# YOUR CODE HERE\n",
    "model = create_model(n_layers=10, first_layer_nodes=500, last_layer_nodes=100, act_funct =\"relu\", negative_node_incrementation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 545)               273045    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 589)               321594    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 634)               374060    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 678)               430530    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 723)               490917    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 767)               555308    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 812)               623616    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 856)               695928    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                8570      \n",
      "=================================================================\n",
      "Total params: 4,166,068\n",
      "Trainable params: 4,166,068\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run model.summary() and make sure that you understand the model architecture that you just built \n",
    "# Notice in the model summary how the number of nodes have been linearly incremented in increasing values.\n",
    "# The output layer must have 10 nodes because there are 10 labels to predict \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to play around with parameters to gain additional insight as to how the create_model function works \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we've played around a bit with  `create_model` in order to understand how it works, let's build a much simpler model that we'll be running gridsearches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model \n",
    "\n",
    "Use `create_model` to build a model. \n",
    "\n",
    "- Set `n_layers = 2` \n",
    "- Set `first_layer_nodes = 500`\n",
    "- Set `last_layer_nodes = 100`\n",
    "- Set `act_funct = \"relu\"`\n",
    "- Make sure that `negative_node_incrementation = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "606b85d0ba4531836f97caf6850297f8",
     "grade": false,
     "grade_id": "cell-4ca6c5e51302fd10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use create_model to create a model \n",
    "\n",
    "model = create_model(n_layers=2, first_layer_nodes=500, last_layer_nodes=100, act_funct =\"relu\", negative_node_incrementation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 397,510\n",
      "Trainable params: 397,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run model.summary() and make sure that you understand the model architecture that you just built \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = {'n_layers': [2, 3],\n",
    "              'epochs': [3], \n",
    "              \"first_layer_nodes\": [500, 300],\n",
    "              \"last_layer_nodes\": [100, 50]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "2021-07-28 14:32:28.272794: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 14:32:28.273846: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 14:32:28.679860: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 14:32:28.680279: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 14:32:28.685303: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 14:32:28.701446: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 14:32:28.773812: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 14:32:31.574164: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 14:32:31.702951: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 14:32:31.916545: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 14:32:32.008098: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 14:32:32.041211: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 14:32:32.051790: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 14:32:32.222775: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "   1/1563 [..............................] - ETA: 33:49 - loss: 2.3709 - accuracy: 0.0938Epoch 1/3\n",
      "   1/1563 [..............................] - ETA: 29:04 - loss: 2.2923 - accuracy: 0.1250Epoch 1/3\n",
      "  60/1563 [>.............................] - ETA: 8s - loss: 1.2736 - accuracy: 0.5844Epoch 1/3\n",
      "  69/1563 [>.............................] - ETA: 8s - loss: 1.2335 - accuracy: 0.5978Epoch 1/3\n",
      "  41/1563 [..............................] - ETA: 6s - loss: 1.4428 - accuracy: 0.5084Epoch 1/3\n",
      " 149/1563 [=>............................] - ETA: 11s - loss: 1.0500 - accuracy: 0.6602Epoch 1/3\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.6620 - accuracy: 0.8028\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.6639 - accuracy: 0.8013\n",
      "1360/1563 [=========================>....] - ETA: 1s - loss: 0.6586 - accuracy: 0.7976Epoch 2/3\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.6585 - accuracy: 0.8025\n",
      "1557/1563 [============================>.] - ETA: 0s - loss: 0.6618 - accuracy: 0.8010Epoch 2/3\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.6612 - accuracy: 0.8011\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.6417 - accuracy: 0.8031\n",
      " 246/1563 [===>..........................] - ETA: 10s - loss: 0.4627 - accuracy: 0.8679Epoch 2/3\n",
      "1563/1563 [==============================] - 15s 9ms/step - loss: 0.6355 - accuracy: 0.8049\n",
      "1477/1563 [===========================>..] - ETA: 0s - loss: 0.6367 - accuracy: 0.8047Epoch 2/3\n",
      "1563/1563 [==============================] - 16s 9ms/step - loss: 0.6302 - accuracy: 0.8070\n",
      " 360/1563 [=====>........................] - ETA: 8s - loss: 0.4721 - accuracy: 0.8586Epoch 2/3\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4503 - accuracy: 0.8647\n",
      "1001/1563 [==================>...........] - ETA: 4s - loss: 0.4300 - accuracy: 0.8698Epoch 3/3\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4506 - accuracy: 0.8670\n",
      "1008/1563 [==================>...........] - ETA: 4s - loss: 0.4302 - accuracy: 0.8696Epoch 3/3\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4496 - accuracy: 0.8662\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4470 - accuracy: 0.8659\n",
      "  52/1563 [..............................] - ETA: 11s - loss: 0.3355 - accuracy: 0.9002Epoch 3/3\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.4376 - accuracy: 0.8656\n",
      " 479/1563 [========>.....................] - ETA: 7s - loss: 0.3538 - accuracy: 0.8948Epoch 3/3\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.4304 - accuracy: 0.8682\n",
      " 531/1563 [=========>....................] - ETA: 7s - loss: 0.3551 - accuracy: 0.8959Epoch 3/3\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.4305 - accuracy: 0.8693\n",
      " 598/1563 [==========>...................] - ETA: 6s - loss: 0.3544 - accuracy: 0.8943Epoch 3/3\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.3601 - accuracy: 0.8930\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.3591 - accuracy: 0.8919\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.3628 - accuracy: 0.8921\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.3600 - accuracy: 0.8920\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.3485 - accuracy: 0.8927\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.3444 - accuracy: 0.8941\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.3415 - accuracy: 0.8949\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4703 - accuracy: 0.8640\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4590 - accuracy: 0.8676\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4675 - accuracy: 0.8648\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4606 - accuracy: 0.8678\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4537 - accuracy: 0.8676\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4671 - accuracy: 0.8641\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4836 - accuracy: 0.8620\n",
      "Epoch 1/3\n",
      "  57/1563 [>.............................] - ETA: 8s - loss: 1.3033 - accuracy: 0.5844Epoch 1/3\n",
      "   6/1563 [..............................] - ETA: 18s - loss: 2.1627 - accuracy: 0.2500  Epoch 1/3\n",
      " 435/1563 [=======>......................] - ETA: 9s - loss: 0.8556 - accuracy: 0.7420Epoch 1/3\n",
      "1365/1563 [=========================>....] - ETA: 2s - loss: 0.6857 - accuracy: 0.7936Epoch 1/3\n",
      " 984/1563 [=================>............] - ETA: 9s - loss: 0.6973 - accuracy: 0.7873Epoch 1/3\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 0.6643 - accuracy: 0.8021\n",
      " 148/1563 [=>............................] - ETA: 23s - loss: 1.0787 - accuracy: 0.6573Epoch 2/3\n",
      "1563/1563 [==============================] - 22s 13ms/step - loss: 0.6680 - accuracy: 0.7997\n",
      "  65/1563 [>.............................] - ETA: 21s - loss: 1.3126 - accuracy: 0.5832Epoch 2/3\n",
      " 238/1563 [===>..........................] - ETA: 19s - loss: 1.0122 - accuracy: 0.6886Epoch 1/3\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.6356 - accuracy: 0.8063\n",
      " 277/1563 [====>.........................] - ETA: 17s - loss: 0.4630 - accuracy: 0.8650Epoch 2/3\n",
      "1563/1563 [==============================] - 28s 15ms/step - loss: 0.6394 - accuracy: 0.8062\n",
      " 696/1563 [============>.................] - ETA: 12s - loss: 0.7509 - accuracy: 0.7663Epoch 2/3\n",
      "1563/1563 [==============================] - 24s 9ms/step - loss: 0.6887 - accuracy: 0.7947\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4516 - accuracy: 0.8649\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4495 - accuracy: 0.8674\n",
      "  57/1563 [>.............................] - ETA: 9s - loss: 0.3577 - accuracy: 0.8904Epoch 3/3\n",
      "1563/1563 [==============================] - 28s 11ms/step - loss: 0.6421 - accuracy: 0.8030\n",
      "1141/1563 [====================>.........] - ETA: 3s - loss: 0.4278 - accuracy: 0.8683Epoch 2/3\n",
      "1563/1563 [==============================] - 21s 8ms/step - loss: 0.6943 - accuracy: 0.7925\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4290 - accuracy: 0.8687\n",
      " 356/1563 [=====>........................] - ETA: 7s - loss: 0.4783 - accuracy: 0.8607Epoch 3/3\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.4324 - accuracy: 0.8673\n",
      " 864/1563 [===============>..............] - ETA: 4s - loss: 0.3557 - accuracy: 0.8932Epoch 3/3\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4719 - accuracy: 0.8603\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.3612 - accuracy: 0.8914\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.3630 - accuracy: 0.8926\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.4760 - accuracy: 0.8593\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.4377 - accuracy: 0.8670\n",
      " 837/1563 [===============>..............] - ETA: 5s - loss: 0.3351 - accuracy: 0.8964Epoch 3/3\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.3421 - accuracy: 0.8943\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.3463 - accuracy: 0.8931\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.4882 - accuracy: 0.8585\n",
      "782/782 [==============================] - 5s 4ms/step - loss: 0.4664 - accuracy: 0.8641\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3887 - accuracy: 0.8832\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3900 - accuracy: 0.8834\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4607 - accuracy: 0.8661\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4568 - accuracy: 0.8645\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3477 - accuracy: 0.8932\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4725 - accuracy: 0.8614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.4659 - accuracy: 0.8648\n",
      "Epoch 1/3\n",
      "  62/1563 [>.............................] - ETA: 2s - loss: 1.2678 - accuracy: 0.6003 6Epoch 1/3\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4583 - accuracy: 0.8651\n",
      " 984/1563 [=================>............] - ETA: 1s - loss: 0.7544 - accuracy: 0.7728Epoch 1/3\n",
      " 210/1563 [===>..........................] - ETA: 5s - loss: 1.0085 - accuracy: 0.6932Epoch 1/3\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6881 - accuracy: 0.7947\n",
      " 274/1563 [====>.........................] - ETA: 6s - loss: 0.9329 - accuracy: 0.7073Epoch 2/3\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6569 - accuracy: 0.7985\n",
      "Epoch 2/3\n",
      "  14/1563 [..............................] - ETA: 6s - loss: 0.3837 - accuracy: 0.8705Epoch 1/3\n",
      " 676/1563 [===========>..................] - ETA: 4s - loss: 0.4765 - accuracy: 0.8580Epoch 1/3\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.6549 - accuracy: 0.7997\n",
      " 783/1563 [==============>...............] - ETA: 3s - loss: 0.7992 - accuracy: 0.7611Epoch 2/3\n",
      " 175/1563 [==>...........................] - ETA: 7s - loss: 0.4721 - accuracy: 0.8562Epoch 1/3\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.6536 - accuracy: 0.8006\n",
      " 755/1563 [=============>................] - ETA: 4s - loss: 0.7833 - accuracy: 0.7651Epoch 2/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4699 - accuracy: 0.8594\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.6893 - accuracy: 0.7944\n",
      " 464/1563 [=======>......................] - ETA: 6s - loss: 0.4378 - accuracy: 0.8671Epoch 2/3\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4491 - accuracy: 0.8621\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 11s 5ms/step - loss: 0.6793 - accuracy: 0.7973\n",
      " 322/1563 [=====>........................] - ETA: 7s - loss: 0.4787 - accuracy: 0.8570Epoch 2/3\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4477 - accuracy: 0.8633\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.6882 - accuracy: 0.7938\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4485 - accuracy: 0.8635\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3847 - accuracy: 0.8848\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4725 - accuracy: 0.8609\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3679 - accuracy: 0.8889\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4648 - accuracy: 0.8621\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4707 - accuracy: 0.8622\n",
      "1477/1563 [===========================>..] - ETA: 0s - loss: 0.3648 - accuracy: 0.8888Epoch 3/3\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.4687 - accuracy: 0.8668\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3653 - accuracy: 0.8886\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3633 - accuracy: 0.8887\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3927 - accuracy: 0.8836\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.4922 - accuracy: 0.8583\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3812 - accuracy: 0.8868\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.4725 - accuracy: 0.8612\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.4651 - accuracy: 0.8622\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3847 - accuracy: 0.8862\n",
      "688/782 [=========================>....] - ETA: 0s - loss: 0.4716 - accuracy: 0.8602Epoch 1/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.4717 - accuracy: 0.8603\n",
      "225/782 [=======>......................] - ETA: 0s - loss: 0.4872 - accuracy: 0.8608Epoch 1/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.4749 - accuracy: 0.8635\n",
      "1028/1563 [==================>...........] - ETA: 1s - loss: 0.7114 - accuracy: 0.7825Epoch 1/3\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6526 - accuracy: 0.8002\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6540 - accuracy: 0.8000\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6573 - accuracy: 0.8014\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4450 - accuracy: 0.8647\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4449 - accuracy: 0.8651\n",
      " 970/1563 [=================>............] - ETA: 1s - loss: 0.4505 - accuracy: 0.8636Epoch 3/3\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4494 - accuracy: 0.8634\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3606 - accuracy: 0.8898\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3653 - accuracy: 0.8885\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4617 - accuracy: 0.8631\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3637 - accuracy: 0.8877\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4593 - accuracy: 0.8648\n",
      "782/782 [==============================] - 1s 873us/step - loss: 0.4643 - accuracy: 0.8642\n",
      "782/782 [==============================] - 1s 681us/step - loss: 0.4898 - accuracy: 0.8576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done  24 out of  24 | elapsed:  2.8min finished\n",
      "2021-07-28 14:35:05.714772: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2344/2344 [==============================] - 4s 1ms/step - loss: 0.6122 - accuracy: 0.8185\n",
      "Epoch 2/3\n",
      "2344/2344 [==============================] - 3s 1ms/step - loss: 0.4228 - accuracy: 0.8739\n",
      "Epoch 3/3\n",
      "2344/2344 [==============================] - 3s 1ms/step - loss: 0.3453 - accuracy: 0.8968\n",
      "Best: 0.865559995174408 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
      "Means: 0.865559995174408, Stdev: 0.0016482652977161257 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
      "Means: 0.8645600080490112, Stdev: 0.0022789651976011295 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
      "Means: 0.863426665465037, Stdev: 0.0037553833491735 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
      "Means: 0.8652399977048238, Stdev: 0.0006424844893858848 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
      "Means: 0.8643200000127157, Stdev: 0.0022349918462418288 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
      "Means: 0.8605599999427795, Stdev: 0.0016714083543717486 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
      "Means: 0.8604666590690613, Stdev: 0.0024353888949524257 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
      "Means: 0.8640533288319906, Stdev: 0.0007305179798447073 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n"
     ]
    }
   ],
   "source": [
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=-2, \n",
    "                    verbose=1, \n",
    "                    cv=3)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 3,\n",
       " 'first_layer_nodes': 500,\n",
       " 'last_layer_nodes': 100,\n",
       " 'n_layers': 2,\n",
       " 'build_fn': <function __main__.create_model(n_layers, first_layer_nodes, last_layer_nodes, act_funct='relu', negative_node_incrementation=True)>}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Experiment 2\n",
    "\n",
    "## Benchmark different Optimization Algorithms \n",
    "\n",
    "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches: \n",
    "\n",
    "- Random Search\n",
    "- Bayesian Optimization. \n",
    "- Brute Force Gridsearch\n",
    "\n",
    "Our goal in this experiment is two-fold. We want to see which appraoch \n",
    "\n",
    "- Scores the highest accuracy\n",
    "- Has the shortest run time \n",
    "\n",
    "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "`Brute Force Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time. \n",
    "\n",
    "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the heightest possible accuracies. \n",
    "\n",
    "`Bayesian Optimization` has a bit of intelligence built into it's search algorithm but you do need to manually select some parameters which greatly influence the model learning outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
    "# let's build a simple model to minimize run time \n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer= tensorflow.keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relu'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "hp = HyperParameters()\n",
    "hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
    "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Run the Gridsearch Algorithms \n",
    "\n",
    "### Random Search\n",
    "\n",
    "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaff9aae33845f374e15f2381719d83a",
     "grade": false,
     "grade_id": "cell-8c1dfb9b6d12bea2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many unique hyperparameter combinations do we have? \n",
    "# HINT: take the product of the number of possible values for each hyperparameter \n",
    "# save your answer to n_unique_hparam_combos\n",
    "\n",
    "# YOUR CODE HERE\n",
    "n_unique_hparam_combos = (512-32)/32 * 3 * 2 # num units, num lr, num activations\n",
    "n_unique_hparam_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9d628451e83431e1b52da10eccf2c00",
     "grade": false,
     "grade_id": "cell-1fa83950bb2d5f92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# how many of these do we want to randomly sample?\n",
    "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
    "# save this number to n_param_combos_to_sample\n",
    "\n",
    "# YOUR CODE HERE\n",
    "n_param_combos_to_sample = n_unique_hparam_combos*0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tuner = RandomSearch(\n",
    "            build_model,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
    "            seed=1234,\n",
    "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
    "            directory='./keras-tuner-trial',\n",
    "            project_name='random_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23 Complete [00h 00m 11s]\n",
      "val_accuracy: 0.8588399887084961\n",
      "\n",
      "Best val_accuracy So Far: 0.871720016002655\n",
      "Total elapsed time: 00h 03m 54s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# take note of Total elapsed time in print out\n",
    "random_tuner.search(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial/random_search\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 352\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.871720016002655\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8683599829673767\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 480\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8635200262069702\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 448\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8617600202560425\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 416\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8614000082015991\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8588399887084961\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 224\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8573200106620789\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 192\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8416799902915955\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 320\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8403599858283997\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 160\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8330000042915344\n"
     ]
    }
   ],
   "source": [
    "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
    "random_tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparamter combination and model score. \n",
    "Note that because this is Random Search, multiple runs might have slighly different outcomes. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f084b5d373f8589a1de8d6d4473b974a",
     "grade": true,
     "grade_id": "cell-5527738b6382c164",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "The best hyperparameters were: \n",
    "units: 352\n",
    "learning_rate: 0.001\n",
    "activation: relu\n",
    "Score: 0.871720016002655"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Bayesian Optimization\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
    "\n",
    "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
    "\n",
    "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
    "\n",
    "`num_initial_points`: \n",
    "\n",
    "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine liklihood of which param combo to try next based on expected improvement\n",
    "\n",
    "\n",
    "`beta`: \n",
    "\n",
    "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent), smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent). \n",
    "\n",
    "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
    "# because BO isn't random (after num_initial_points number of trails) let's see if 15 max trials gives good results\n",
    "# feel free to play with any of these numbers\n",
    "max_trials=15\n",
    "num_initial_points=5\n",
    "beta=5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_tuner = BayesianOptimization(\n",
    "                    build_model,\n",
    "                    objective='val_accuracy',\n",
    "                    max_trials=max_trials,\n",
    "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
    "                    num_initial_points=num_initial_points, \n",
    "                    beta=beta, \n",
    "                    seed=1234,\n",
    "                    directory='./keras-tuner-trial',\n",
    "                    project_name='bayesian_optimization_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 00m 07s]\n",
      "val_accuracy: 0.8206400275230408\n",
      "\n",
      "Best val_accuracy So Far: 0.8754000067710876\n",
      "Total elapsed time: 00h 02m 45s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner.search(X_train, y_train,\n",
    "               epochs=3,\n",
    "               validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 352\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8754000067710876\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8732399940490723\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 480\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8648800253868103\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.001\n",
      "activation: sigmoid\n",
      "Score: 0.8584799766540527\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 320\n",
      "learning_rate: 0.01\n",
      "activation: sigmoid\n",
      "Score: 0.8392400145530701\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8268399834632874\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8250799775123596\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.01\n",
      "activation: relu\n",
      "Score: 0.8224800229072571\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.001\n",
      "activation: relu\n",
      "Score: 0.8206400275230408\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 512\n",
      "learning_rate: 0.01\n",
      "activation: relu\n",
      "Score: 0.8183599710464478\n"
     ]
    }
   ],
   "source": [
    "bayesian_tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparamter combination and model score. \n",
    "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
     "grade": true,
     "grade_id": "cell-ff95600bf745f40f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "Best performing hyperparamter combo and score:\n",
    "\n",
    "units: 352\n",
    "learning_rate: 0.001\n",
    "activation: relu\n",
    "Score: 0.8754000067710876"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Brute Force Gridsearch Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate a Sklearn compatiable parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build out our hyperparameter dictionary \n",
    "hyper_parameters = {\n",
    "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
    "    \"units\": np.arange(32, 544, 32).tolist(),\n",
    "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
    "    \"activation\":[\"relu\", \"sigmoid\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units': [32,\n",
       "  64,\n",
       "  96,\n",
       "  128,\n",
       "  160,\n",
       "  192,\n",
       "  224,\n",
       "  256,\n",
       "  288,\n",
       "  320,\n",
       "  352,\n",
       "  384,\n",
       "  416,\n",
       "  448,\n",
       "  480,\n",
       "  512],\n",
       " 'learning_rate': [0.1, 0.01, 0.001],\n",
       " 'activation': ['relu', 'sigmoid']}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Sklearn compatiable model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(units, learning_rate, activation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a complie keras model ready for keras-tuner gridsearch algorithms \n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # hidden layer\n",
    "    model.add(Dense(units, activation=activation))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn = build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "2021-07-28 15:00:47.122910: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 15:00:47.122895: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 15:00:47.500294: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 15:00:47.830105: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 15:00:47.840323: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 15:00:47.853420: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 15:00:47.855107: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-28 15:00:50.199169: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 15:00:50.379108: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 15:00:50.663067: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 15:00:50.714867: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 15:00:50.838999: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 15:00:50.881689: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-28 15:00:50.901320: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.8358 - accuracy: 0.3267\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0519 - accuracy: 0.2322\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9538 - accuracy: 0.3029\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9261 - accuracy: 0.3139\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8221 - accuracy: 0.3433\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0105 - accuracy: 0.2653\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9635 - accuracy: 0.2826\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.1122 - accuracy: 0.2218\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.9440 - accuracy: 0.2653\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.0244 - accuracy: 0.2395\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.9703 - accuracy: 0.2456\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.8454 - accuracy: 0.3268\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.0871 - accuracy: 0.2048\n",
      "782/782 [==============================] - 2s 1ms/step - loss: 1.9705 - accuracy: 0.2568\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8579 - accuracy: 0.3509\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8484 - accuracy: 0.3408\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9766 - accuracy: 0.3098\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9014 - accuracy: 0.3287\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9813 - accuracy: 0.3039\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9889 - accuracy: 0.3072\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.8277 - accuracy: 0.3133\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.0218 - accuracy: 0.2760\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.0279 - accuracy: 0.2294\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.0628 - accuracy: 0.2621\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9697 - accuracy: 0.3188\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 1.9564 - accuracy: 0.2746\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.0872 - accuracy: 0.2267\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 2.4435 - accuracy: 0.3260\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9269 - accuracy: 0.3365\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9768 - accuracy: 0.3210\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9332 - accuracy: 0.3347\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9971 - accuracy: 0.3135\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 1.9755 - accuracy: 0.3598\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.8924 - accuracy: 0.3143\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 1.9484 - accuracy: 0.3664\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.8943 - accuracy: 0.2878\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.8435 - accuracy: 0.3228\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.8493 - accuracy: 0.2939\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9919 - accuracy: 0.3617\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 1.8415 - accuracy: 0.3287\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 1.9449 - accuracy: 0.2590\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.9530 - accuracy: 0.3048\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0190 - accuracy: 0.3413\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0301 - accuracy: 0.3119\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.9842 - accuracy: 0.3396\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1988 - accuracy: 0.2417\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0835 - accuracy: 0.3264\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.9266 - accuracy: 0.3500\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9357 - accuracy: 0.3628\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.0605 - accuracy: 0.2252\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.1346 - accuracy: 0.1677\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 2.0278 - accuracy: 0.3197\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8062 - accuracy: 0.3178\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 1.8366 - accuracy: 0.3325\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.9091 - accuracy: 0.2877\n",
      "782/782 [==============================] - 10s 9ms/step - loss: 2.0741 - accuracy: 0.2191\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 2.0137 - accuracy: 0.3219\n",
      "1563/1563 [==============================] - 18s 10ms/step - loss: 1.9598 - accuracy: 0.3814\n",
      "1563/1563 [==============================] - 19s 11ms/step - loss: 2.0981 - accuracy: 0.3357\n",
      "1563/1563 [==============================] - 15s 6ms/step - loss: 2.1375 - accuracy: 0.3281\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.8520 - accuracy: 0.3137\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 2.0516 - accuracy: 0.3262\n",
      "1563/1563 [==============================] - 11s 4ms/step - loss: 1.9724 - accuracy: 0.3751\n",
      "1563/1563 [==============================] - 11s 4ms/step - loss: 2.1313 - accuracy: 0.3253\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.9598 - accuracy: 0.2619\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.8774 - accuracy: 0.2890\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.9010 - accuracy: 0.3484\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.8901 - accuracy: 0.2930\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 2.1684 - accuracy: 0.3159\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 2.1140 - accuracy: 0.3217\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.8633 - accuracy: 0.2929\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 2.1031 - accuracy: 0.3563\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.0755 - accuracy: 0.3389\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.0466 - accuracy: 0.3329\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.9324 - accuracy: 0.2732\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 1.8050 - accuracy: 0.3296\n",
      "1118/1563 [====================>.........] - ETA: 1s - loss: 2.1067 - accuracy: 0.3655"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed:  1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 9s 4ms/step - loss: 2.0629 - accuracy: 0.3516\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.9523 - accuracy: 0.2118\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 2.1829 - accuracy: 0.3128\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.9482 - accuracy: 0.3014\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.8434 - accuracy: 0.3009\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.9655 - accuracy: 0.2463\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 2.3198 - accuracy: 0.3219\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.9727 - accuracy: 0.3274\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.2830 - accuracy: 0.3034\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1477 - accuracy: 0.3585\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 2.1594 - accuracy: 0.3483\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 1.9460 - accuracy: 0.3289\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 1.9348 - accuracy: 0.2738\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 1.9088 - accuracy: 0.3139\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.0710 - accuracy: 0.3527\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.1146 - accuracy: 0.3395\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.4067 - accuracy: 0.3117\n",
      "1563/1563 [==============================] - 6s 2ms/step - loss: 0.8018 - accuracy: 0.7576\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.9092 - accuracy: 0.3152\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.9373 - accuracy: 0.2814\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.8587 - accuracy: 0.3162\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.7075 - accuracy: 0.7893\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.8015 - accuracy: 0.7563\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.7839 - accuracy: 0.7619\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7551 - accuracy: 0.7724\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.7262 - accuracy: 0.7840\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.7325 - accuracy: 0.7834\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.7303 - accuracy: 0.7798\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.7620 - accuracy: 0.7693\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.7716 - accuracy: 0.7661\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7484 - accuracy: 0.7732\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7484 - accuracy: 0.7727\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6704 - accuracy: 0.8019\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6670 - accuracy: 0.8018\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6600 - accuracy: 0.8043\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6591 - accuracy: 0.7987\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.7653 - accuracy: 0.7658\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.7409 - accuracy: 0.7728\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.7474 - accuracy: 0.7731\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6918 - accuracy: 0.8011\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7457 - accuracy: 0.7721\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6503 - accuracy: 0.8070\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6924 - accuracy: 0.7958\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7363 - accuracy: 0.7771\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7477 - accuracy: 0.7749\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6806 - accuracy: 0.7995\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7532 - accuracy: 0.7714\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6577 - accuracy: 0.8098\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.6805 - accuracy: 0.8010\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6918 - accuracy: 0.7929\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7411 - accuracy: 0.7778\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7510 - accuracy: 0.7722\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7389 - accuracy: 0.7762\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6439 - accuracy: 0.8142\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7428 - accuracy: 0.7751\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6583 - accuracy: 0.8059\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7473 - accuracy: 0.7730\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6357 - accuracy: 0.8154\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6928 - accuracy: 0.7941\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.6855 - accuracy: 0.7972\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7410 - accuracy: 0.7761\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7445 - accuracy: 0.7768\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6861 - accuracy: 0.8042\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7403 - accuracy: 0.7740\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7410 - accuracy: 0.7763\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6587 - accuracy: 0.8048\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7391 - accuracy: 0.7770\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6852 - accuracy: 0.7952\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6492 - accuracy: 0.8087\n",
      "782/782 [==============================] - 3s 2ms/step - loss: 0.6875 - accuracy: 0.7949\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7442 - accuracy: 0.7742\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7536 - accuracy: 0.7723\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7372 - accuracy: 0.7784\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6828 - accuracy: 0.7954\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6565 - accuracy: 0.8101\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7470 - accuracy: 0.7761\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6621 - accuracy: 0.8006\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 0.7391 - accuracy: 0.7772\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.6340 - accuracy: 0.8136\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7462 - accuracy: 0.7770\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7526 - accuracy: 0.7757\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.7032 - accuracy: 0.7946\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6734 - accuracy: 0.8099\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6978 - accuracy: 0.7912\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7466 - accuracy: 0.7762\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 0.7425 - accuracy: 0.7763\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7510 - accuracy: 0.7744\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6643 - accuracy: 0.8026\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6565 - accuracy: 0.8027\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 0.7461 - accuracy: 0.7725\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6861 - accuracy: 0.7948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.7365 - accuracy: 0.7786\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7500 - accuracy: 0.7757\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7461 - accuracy: 0.7754\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6564 - accuracy: 0.8058\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.6499 - accuracy: 0.8062\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.6509 - accuracy: 0.8075\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7482 - accuracy: 0.7778\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7488 - accuracy: 0.7767\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.6702 - accuracy: 0.7977\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7495 - accuracy: 0.7758\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.6910 - accuracy: 0.8022\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7371 - accuracy: 0.7792\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6770 - accuracy: 0.8014\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 0.7468 - accuracy: 0.7763\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.6519 - accuracy: 0.8031\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.6977 - accuracy: 0.7882\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7485 - accuracy: 0.7751\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6876 - accuracy: 0.7941\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7524 - accuracy: 0.7747\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7395 - accuracy: 0.7778\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.7075 - accuracy: 0.7892\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7516 - accuracy: 0.7737\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6739 - accuracy: 0.7992\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.8813 - accuracy: 0.7382\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.8701 - accuracy: 0.7398\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.8008\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.8610 - accuracy: 0.7459\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.7178 - accuracy: 0.7924\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.7509 - accuracy: 0.7843\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6418 - accuracy: 0.8094\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8125 - accuracy: 0.7590\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.7108 - accuracy: 0.7934\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7983 - accuracy: 0.7630 \n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6583 - accuracy: 0.8082\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.8058 - accuracy: 0.7614\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6474 - accuracy: 0.8124\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.7793 - accuracy: 0.7696\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.7652 - accuracy: 0.7723\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.7640 - accuracy: 0.7743\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6583 - accuracy: 0.8084\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6265 - accuracy: 0.8123\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6245 - accuracy: 0.8153\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 0.7407 - accuracy: 0.7811\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7454 - accuracy: 0.7782\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6360 - accuracy: 0.8095\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5972 - accuracy: 0.8233 \n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5884 - accuracy: 0.8294\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7499 - accuracy: 0.7772\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7239 - accuracy: 0.7851\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5869 - accuracy: 0.8306\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7409 - accuracy: 0.7796\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7253 - accuracy: 0.7833\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5891 - accuracy: 0.8250\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7147 - accuracy: 0.7883\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5862 - accuracy: 0.8273\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5896 - accuracy: 0.8269\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7175 - accuracy: 0.7846\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7135 - accuracy: 0.7884\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5690 - accuracy: 0.8354\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7025 - accuracy: 0.7917\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5828 - accuracy: 0.8295\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5646 - accuracy: 0.8373\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7069 - accuracy: 0.7898\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5778 - accuracy: 0.8278\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7004 - accuracy: 0.7916\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6952 - accuracy: 0.7934\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5501 - accuracy: 0.8385\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7045 - accuracy: 0.7911\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5439 - accuracy: 0.8427\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5628 - accuracy: 0.8337\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6992 - accuracy: 0.7916\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6922 - accuracy: 0.7943\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5478 - accuracy: 0.8369\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5607 - accuracy: 0.8332\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5529 - accuracy: 0.8357\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6891 - accuracy: 0.7950\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6934 - accuracy: 0.7924\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5350 - accuracy: 0.8446\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6874 - accuracy: 0.7959\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6858 - accuracy: 0.7947\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5537 - accuracy: 0.8380\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6891 - accuracy: 0.7952\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5538 - accuracy: 0.8393\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6778 - accuracy: 0.7966\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 0.6815 - accuracy: 0.7963\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5468 - accuracy: 0.8408\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6792 - accuracy: 0.7958\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5532 - accuracy: 0.8369\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5584 - accuracy: 0.8353\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5445 - accuracy: 0.8368\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5482 - accuracy: 0.8365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6741 - accuracy: 0.7987\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6799 - accuracy: 0.7972\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5576 - accuracy: 0.8344\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6805 - accuracy: 0.7964\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5469 - accuracy: 0.8368\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6688 - accuracy: 0.8007\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6756 - accuracy: 0.7984\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5488 - accuracy: 0.8364\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6748 - accuracy: 0.7979\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6697 - accuracy: 0.8014\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5489 - accuracy: 0.8338\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5281 - accuracy: 0.8452\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5408 - accuracy: 0.8435\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5390 - accuracy: 0.8382\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6740 - accuracy: 0.7978\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6706 - accuracy: 0.7982\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6625 - accuracy: 0.8021\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6673 - accuracy: 0.8015\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6647 - accuracy: 0.8005\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5444 - accuracy: 0.8445\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5375 - accuracy: 0.8394\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6608 - accuracy: 0.8028\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5505 - accuracy: 0.8354\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5264 - accuracy: 0.8449\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5506 - accuracy: 0.8371\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6668 - accuracy: 0.8023\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5361 - accuracy: 0.8418\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5193 - accuracy: 0.8433\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1292 - accuracy: 0.6372\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1571 - accuracy: 0.6260\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6621 - accuracy: 0.8012\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1406 - accuracy: 0.6419\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 1.1591 - accuracy: 0.6326\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.0882 - accuracy: 0.6658\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.0550 - accuracy: 0.6502\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5336 - accuracy: 0.8447\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 1.1778 - accuracy: 0.6260\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 1.1177 - accuracy: 0.6442\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 1.0843 - accuracy: 0.6572\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 1.1103 - accuracy: 0.6452\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2362 - accuracy: 0.6058\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.0293 - accuracy: 0.6866\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2087 - accuracy: 0.6319\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2461 - accuracy: 0.6171\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2868 - accuracy: 0.6012\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1891 - accuracy: 0.6335\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.2054 - accuracy: 0.6420\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2086 - accuracy: 0.6305\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.4144 - accuracy: 0.5978\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.1714 - accuracy: 0.6390\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1321 - accuracy: 0.6481\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.1186 - accuracy: 0.6652\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.2567 - accuracy: 0.5922\n",
      "1563/1563 [==============================] - 5s 2ms/step - loss: 1.3053 - accuracy: 0.6100\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.0124 - accuracy: 0.6907\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.4447 - accuracy: 0.5638\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4258 - accuracy: 0.5866\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2518 - accuracy: 0.6201\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5174 - accuracy: 0.5905\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.3692 - accuracy: 0.5968\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2209 - accuracy: 0.6232\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.3602 - accuracy: 0.6042\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2252 - accuracy: 0.6168\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.6800 - accuracy: 0.5779\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5532 - accuracy: 0.5820\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.0555 - accuracy: 0.6786\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.2734 - accuracy: 0.6126\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3828 - accuracy: 0.6118\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.3944 - accuracy: 0.6493\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2130 - accuracy: 0.6450\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5855 - accuracy: 0.5616\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.4603 - accuracy: 0.5940\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 1.4560 - accuracy: 0.6007\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.7505 - accuracy: 0.5461\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.5929 - accuracy: 0.5847\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 1.5013 - accuracy: 0.5728\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.3191 - accuracy: 0.5940\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.5026 - accuracy: 0.5827\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.4065 - accuracy: 0.5942\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.8292 - accuracy: 0.5470\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.2377 - accuracy: 0.6418\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.2899 - accuracy: 0.6206\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.4482 - accuracy: 0.6128\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.4001 - accuracy: 0.6112\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.0370 - accuracy: 0.6877\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 1.4104 - accuracy: 0.6112\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5223 - accuracy: 0.5983\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.3609 - accuracy: 0.6025\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 1.5708 - accuracy: 0.5756\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 1.8016 - accuracy: 0.5688\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2672 - accuracy: 0.6587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 1.5023 - accuracy: 0.6025\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 1.4743 - accuracy: 0.6128\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.3272 - accuracy: 0.6119\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.9524 - accuracy: 0.5863\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.2237 - accuracy: 0.6538\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5990 - accuracy: 0.5853\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1996 - accuracy: 0.6378\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5336 - accuracy: 0.5850\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 1.4563 - accuracy: 0.6095\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 1.8288 - accuracy: 0.5828\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0549 - accuracy: 0.6752\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 1.5304 - accuracy: 0.5911\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 1.2832 - accuracy: 0.6183\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.5944 - accuracy: 0.6117\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.1436 - accuracy: 0.5385\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9512 - accuracy: 0.5523\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.4568 - accuracy: 0.6386\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.0210 - accuracy: 0.6887\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.5320 - accuracy: 0.4898\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0125 - accuracy: 0.5557\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3429 - accuracy: 0.6143\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.5421 - accuracy: 0.4823\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.2165 - accuracy: 0.5566\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0147 - accuracy: 0.5599\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.3083 - accuracy: 0.6403\n",
      "1373/1563 [=========================>....] - ETA: 0s - loss: 1.4217 - accuracy: 0.6095"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done 186 tasks      | elapsed:  6.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 8s 4ms/step - loss: 1.4153 - accuracy: 0.6128\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.4532 - accuracy: 0.5019\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 2.6317 - accuracy: 0.5348\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 1.8687 - accuracy: 0.5568\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8524 - accuracy: 0.5961\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.2158 - accuracy: 0.6482\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.8141 - accuracy: 0.5962\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 1.4419 - accuracy: 0.6062\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.3816 - accuracy: 0.6651\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7906 - accuracy: 0.7623\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.4085 - accuracy: 0.6453\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7955 - accuracy: 0.7583\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8045 - accuracy: 0.7538\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.6923 - accuracy: 0.7907\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7310 - accuracy: 0.7772\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.7184 - accuracy: 0.7844\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6928 - accuracy: 0.7912\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7316 - accuracy: 0.7759\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6926 - accuracy: 0.7875\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7387 - accuracy: 0.7758\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 0.7091 - accuracy: 0.7811\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6249 - accuracy: 0.8127\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6648 - accuracy: 0.8008\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.7116 - accuracy: 0.7808\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6277 - accuracy: 0.8099\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.7137 - accuracy: 0.7813\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.7011 - accuracy: 0.7856\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6565 - accuracy: 0.7993\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 0.7021 - accuracy: 0.7842\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6102 - accuracy: 0.8147\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6111 - accuracy: 0.8139\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 0.7032 - accuracy: 0.7837\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6149 - accuracy: 0.8165\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 0.6872 - accuracy: 0.7902\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6970 - accuracy: 0.7866\n",
      "782/782 [==============================] - 3s 2ms/step - loss: 0.6045 - accuracy: 0.8182\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6956 - accuracy: 0.7864\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6903 - accuracy: 0.7891\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6920 - accuracy: 0.7869\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5991 - accuracy: 0.8166\n",
      "782/782 [==============================] - 3s 2ms/step - loss: 0.6118 - accuracy: 0.8130\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6077 - accuracy: 0.8142\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6099 - accuracy: 0.8142\n",
      "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6914 - accuracy: 0.7866\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6920 - accuracy: 0.7864\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6030 - accuracy: 0.8148\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6014 - accuracy: 0.8185\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6868 - accuracy: 0.7874\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5877 - accuracy: 0.8226\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 0.6899 - accuracy: 0.7875\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5753 - accuracy: 0.8308\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.6870 - accuracy: 0.7897\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 0.6842 - accuracy: 0.7905\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5845 - accuracy: 0.8260\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6151 - accuracy: 0.8139\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5914 - accuracy: 0.8186\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6924 - accuracy: 0.7859\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6876 - accuracy: 0.7894\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6870 - accuracy: 0.7891\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6041 - accuracy: 0.8203\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6381 - accuracy: 0.8017\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6951 - accuracy: 0.7868\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5752 - accuracy: 0.8268\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6805 - accuracy: 0.7907\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5862 - accuracy: 0.8259\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6892 - accuracy: 0.7901\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.6857 - accuracy: 0.7887\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6413 - accuracy: 0.8061\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6201 - accuracy: 0.8114\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.6877 - accuracy: 0.7906\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6135 - accuracy: 0.8138\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6896 - accuracy: 0.7879\n",
      "782/782 [==============================] - 5s 4ms/step - loss: 0.6353 - accuracy: 0.8102\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 0.6899 - accuracy: 0.7879\n",
      "1563/1563 [==============================] - 11s 5ms/step - loss: 0.6895 - accuracy: 0.7889\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5848 - accuracy: 0.8241\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5855 - accuracy: 0.8298\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.6934 - accuracy: 0.7867\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6148 - accuracy: 0.8054\n",
      "1563/1563 [==============================] - 11s 5ms/step - loss: 0.6949 - accuracy: 0.7880\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6254 - accuracy: 0.8146\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.6778 - accuracy: 0.7908\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5995 - accuracy: 0.8165\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.6889 - accuracy: 0.7882\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6490 - accuracy: 0.8042\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.6926 - accuracy: 0.7874\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6036 - accuracy: 0.8192\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.6921 - accuracy: 0.7869\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.6914 - accuracy: 0.7879\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.5908 - accuracy: 0.8244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 11s 6ms/step - loss: 0.6957 - accuracy: 0.7872\n",
      "782/782 [==============================] - 6s 6ms/step - loss: 0.6151 - accuracy: 0.8091\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.6546 - accuracy: 0.8034\n",
      "1563/1563 [==============================] - 13s 7ms/step - loss: 0.6838 - accuracy: 0.7908\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.5978 - accuracy: 0.8197\n",
      "1563/1563 [==============================] - 14s 7ms/step - loss: 0.6977 - accuracy: 0.7865\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6491 - accuracy: 0.8027\n",
      "1563/1563 [==============================] - 12s 5ms/step - loss: 0.6962 - accuracy: 0.7866\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6061 - accuracy: 0.8187\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6042 - accuracy: 0.8184\n",
      "1563/1563 [==============================] - 11s 5ms/step - loss: 0.6886 - accuracy: 0.7887\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 0.6983 - accuracy: 0.7884\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 1.0481 - accuracy: 0.6988\n",
      "1563/1563 [==============================] - 10s 4ms/step - loss: 0.6984 - accuracy: 0.7857\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.5874 - accuracy: 0.8239 \n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6041 - accuracy: 0.8184\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 1.0537 - accuracy: 0.6992\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.8196 - accuracy: 0.7554\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6048 - accuracy: 0.8199     \n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 1.0596 - accuracy: 0.7004\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.8157 - accuracy: 0.7619\n",
      "1563/1563 [==============================] - 8s 3ms/step - loss: 0.9414 - accuracy: 0.7268\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.8260 - accuracy: 0.7553\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.9425 - accuracy: 0.7269\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.7525 - accuracy: 0.77630e+\n",
      "1563/1563 [==============================] - 8s 3ms/step - loss: 0.9389 - accuracy: 0.7247\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7512 - accuracy: 0.7816\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8901 - accuracy: 0.7428\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.7462 - accuracy: 0.7794\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8977 - accuracy: 0.7370\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 0.8951 - accuracy: 0.7370\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.7292 - accuracy: 0.7814\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.7267 - accuracy: 0.7887\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8743 - accuracy: 0.7425\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.7160 - accuracy: 0.7891\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 0.8718 - accuracy: 0.7441\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.8711 - accuracy: 0.7449\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.7164 - accuracy: 0.7852\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.7073 - accuracy: 0.7969\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 0.8539 - accuracy: 0.7481\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.7085 - accuracy: 0.7882\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8557 - accuracy: 0.7465\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 0.7079 - accuracy: 0.7887\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8677 - accuracy: 0.7448\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 0.8507 - accuracy: 0.7492\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6927 - accuracy: 0.7973\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6985 - accuracy: 0.7914\n",
      "1563/1563 [==============================] - 9s 4ms/step - loss: 0.8512 - accuracy: 0.7490\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6983 - accuracy: 0.7910\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.8514 - accuracy: 0.7488\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.8394 - accuracy: 0.7519\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6980 - accuracy: 0.7951\n",
      "1563/1563 [==============================] - 11s 5ms/step - loss: 0.8487 - accuracy: 0.7503\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6946 - accuracy: 0.7954\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6917 - accuracy: 0.7940\n",
      "1563/1563 [==============================] - 11s 5ms/step - loss: 0.8430 - accuracy: 0.7514\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6920 - accuracy: 0.8001\n",
      "1563/1563 [==============================] - 11s 5ms/step - loss: 0.8285 - accuracy: 0.7550\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6915 - accuracy: 0.7983\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.8350 - accuracy: 0.7518\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.7008 - accuracy: 0.7876\n",
      "1563/1563 [==============================] - 13s 6ms/step - loss: 0.8393 - accuracy: 0.7515\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6747 - accuracy: 0.8028\n",
      "1563/1563 [==============================] - 11s 5ms/step - loss: 0.8265 - accuracy: 0.7552\n",
      "1563/1563 [==============================] - 11s 5ms/step - loss: 0.8327 - accuracy: 0.7522\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6795 - accuracy: 0.7993\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6798 - accuracy: 0.7981\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6827 - accuracy: 0.8022\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 0.8341 - accuracy: 0.7517\n",
      "1563/1563 [==============================] - 12s 6ms/step - loss: 0.8217 - accuracy: 0.7560\n",
      "1563/1563 [==============================] - 13s 6ms/step - loss: 0.8273 - accuracy: 0.7533\n",
      "1563/1563 [==============================] - 12s 5ms/step - loss: 0.8295 - accuracy: 0.7530\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6730 - accuracy: 0.8011\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6907 - accuracy: 0.7956\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6710 - accuracy: 0.8071\n",
      "1563/1563 [==============================] - 11s 5ms/step - loss: 0.8155 - accuracy: 0.7569\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6858 - accuracy: 0.7931\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.8227 - accuracy: 0.7574\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.8306 - accuracy: 0.7531\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6762 - accuracy: 0.7989\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6693 - accuracy: 0.8055\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6704 - accuracy: 0.8046\n",
      "1563/1563 [==============================] - 12s 6ms/step - loss: 0.8213 - accuracy: 0.7563\n",
      "1563/1563 [==============================] - 13s 7ms/step - loss: 0.8251 - accuracy: 0.7567\n",
      "1563/1563 [==============================] - 14s 7ms/step - loss: 0.8230 - accuracy: 0.7543\n",
      "1563/1563 [==============================] - 14s 6ms/step - loss: 0.8197 - accuracy: 0.7576\n",
      "782/782 [==============================] - 5s 4ms/step - loss: 0.6950 - accuracy: 0.78613\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6720 - accuracy: 0.8033\n",
      "1563/1563 [==============================] - 12s 5ms/step - loss: 0.8218 - accuracy: 0.7554\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6890 - accuracy: 0.7962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6777 - accuracy: 0.7941\n",
      "1563/1563 [==============================] - 11s 5ms/step - loss: 0.8232 - accuracy: 0.7553\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.8136 - accuracy: 0.7576\n",
      "782/782 [==============================] - 5s 4ms/step - loss: 0.6805 - accuracy: 0.7987\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6667 - accuracy: 0.8014\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6747 - accuracy: 0.8009\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.8183 - accuracy: 0.7547\n",
      "1563/1563 [==============================] - 11s 6ms/step - loss: 0.8188 - accuracy: 0.7546\n",
      "1563/1563 [==============================] - 13s 6ms/step - loss: 0.8144 - accuracy: 0.7569\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6716 - accuracy: 0.8030\n",
      "1563/1563 [==============================] - 12s 6ms/step - loss: 0.8207 - accuracy: 0.7542\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.6627 - accuracy: 0.8042\n",
      "1563/1563 [==============================] - 13s 6ms/step - loss: 0.8239 - accuracy: 0.7527\n",
      "782/782 [==============================] - 5s 4ms/step - loss: 0.6730 - accuracy: 0.8005\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6681 - accuracy: 0.8046\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.6739 - accuracy: 0.8031\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.8141 - accuracy: 0.7567\n",
      "1563/1563 [==============================] - 10s 5ms/step - loss: 0.8163 - accuracy: 0.7552\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6790 - accuracy: 0.7968\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6609 - accuracy: 0.8061\n",
      "1563/1563 [==============================] - 8s 4ms/step - loss: 0.8173 - accuracy: 0.7560\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.6697 - accuracy: 0.8028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done 288 out of 288 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2344/2344 [==============================] - 8s 3ms/step - loss: 0.6117 - accuracy: 0.8184\n",
      "Best: 0.8432666659355164 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
      "Means: 0.2511066645383835, Stdev: 0.05394120367958035 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
      "Means: 0.2558933347463608, Stdev: 0.008041387373901181 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
      "Means: 0.27626666923364, Stdev: 0.030129509293284954 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
      "Means: 0.25540000200271606, Stdev: 0.019054809460001487 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
      "Means: 0.29184000194072723, Stdev: 0.04609313914258724 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
      "Means: 0.2986533244450887, Stdev: 0.011356263602729911 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
      "Means: 0.2974933385848999, Stdev: 0.028912123202692836 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
      "Means: 0.24761333564917246, Stdev: 0.0760743846965991 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
      "Means: 0.3126800060272217, Stdev: 0.018633912564099544 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
      "Means: 0.2863333324591319, Stdev: 0.04783092719923697 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
      "Means: 0.29973334074020386, Stdev: 0.03611782324972325 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
      "Means: 0.2863866686820984, Stdev: 0.009296155967534907 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
      "Means: 0.2625733365615209, Stdev: 0.04941408986004428 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
      "Means: 0.309906671444575, Stdev: 0.01239923223982314 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
      "Means: 0.30556000272432965, Stdev: 0.0232503557269773 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
      "Means: 0.30426666140556335, Stdev: 0.016145651604866734 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 512}\n",
      "Means: 0.7855733235677084, Stdev: 0.0026340039119628713 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
      "Means: 0.7945066690444946, Stdev: 0.010370957837883214 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
      "Means: 0.8013733228047689, Stdev: 0.002295570431724172 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
      "Means: 0.8007599910100301, Stdev: 0.00467172950971517 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
      "Means: 0.8012266755104065, Stdev: 0.006909432753121791 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
      "Means: 0.8118399977684021, Stdev: 0.004240872893742676 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
      "Means: 0.7984933455785116, Stdev: 0.004253590106962256 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
      "Means: 0.8028799891471863, Stdev: 0.005661473520512095 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
      "Means: 0.8001600106557211, Stdev: 0.007045965146246601 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
      "Means: 0.802946666876475, Stdev: 0.007938181317124112 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
      "Means: 0.8012400070826212, Stdev: 0.0076702900533169305 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
      "Means: 0.8011066714922587, Stdev: 0.004610653709869126 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
      "Means: 0.8038133382797241, Stdev: 0.004342196418338058 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
      "Means: 0.8022133509318033, Stdev: 0.000719508168421489 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
      "Means: 0.7904933492342631, Stdev: 0.002568805630894533 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
      "Means: 0.8031333486239115, Stdev: 0.004453542672372691 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 512}\n",
      "Means: 0.7900399963061014, Stdev: 0.004062051567923702 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
      "Means: 0.809666653474172, Stdev: 0.0019634821790009576 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
      "Means: 0.812386671702067, Stdev: 0.0023683130260078 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
      "Means: 0.8277866641680399, Stdev: 0.0031937357412752053 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
      "Means: 0.826426645119985, Stdev: 0.0009940240290155949 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
      "Means: 0.8340399861335754, Stdev: 0.003318307061654946 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
      "Means: 0.8363333145777384, Stdev: 0.006300079154313973 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
      "Means: 0.8346133232116699, Stdev: 0.0016409444718420792 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
      "Means: 0.8394400080045065, Stdev: 0.0037737418142075735 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
      "Means: 0.8384933272997538, Stdev: 0.0023281047427025146 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
      "Means: 0.8367466727892557, Stdev: 0.00016759581357223187 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
      "Means: 0.835866649945577, Stdev: 0.0010194491663403447 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
      "Means: 0.8408400019009908, Stdev: 0.005025046973283849 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
      "Means: 0.840666651725769, Stdev: 0.0027405724009255707 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
      "Means: 0.8391466538111368, Stdev: 0.004142308327479916 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
      "Means: 0.8432666659355164, Stdev: 0.0011763750524936263 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
      "Means: 0.6577199896176656, Stdev: 0.006395566256135772 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
      "Means: 0.6586533387502035, Stdev: 0.019765855460571777 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
      "Means: 0.6262666781743368, Stdev: 0.020195420635126943 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
      "Means: 0.6493600010871887, Stdev: 0.041766806536707606 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
      "Means: 0.5949199994405111, Stdev: 0.022618542292922497 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
      "Means: 0.6230266690254211, Stdev: 0.041795592448872716 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
      "Means: 0.6134533286094666, Stdev: 0.047672175001460086 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
      "Means: 0.5745466550191244, Stdev: 0.020039165626398042 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
      "Means: 0.6474399964014689, Stdev: 0.030844868122062337 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
      "Means: 0.6212399999300638, Stdev: 0.02647409332573397 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
      "Means: 0.6173333326975504, Stdev: 0.027849513102040254 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
      "Means: 0.6239466667175293, Stdev: 0.03782528589759577 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
      "Means: 0.621946652730306, Stdev: 0.06246073566555234 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
      "Means: 0.5374666750431061, Stdev: 0.07276411069177743 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
      "Means: 0.5616399844487509, Stdev: 0.06266878222425767 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
      "Means: 0.6355200012524923, Stdev: 0.028977256798903648 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 512}\n",
      "Means: 0.7887599865595499, Stdev: 0.0030609840705633465 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
      "Means: 0.8003200093905131, Stdev: 0.010293458418891933 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
      "Means: 0.8079466621081034, Stdev: 0.006433938214442502 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
      "Means: 0.816213329633077, Stdev: 0.001793104914746504 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
      "Means: 0.8145999908447266, Stdev: 0.0014645864277095845 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
      "Means: 0.8158400058746338, Stdev: 0.0018849662145623916 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
      "Means: 0.8264266649881998, Stdev: 0.003363857307665555 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
      "Means: 0.8176266749699911, Stdev: 0.0027092528240811185 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
      "Means: 0.818120002746582, Stdev: 0.011629874459191709 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
      "Means: 0.8103999892870585, Stdev: 0.003207991412878917 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
      "Means: 0.821399986743927, Stdev: 0.008229585820136026 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
      "Means: 0.8121599952379862, Stdev: 0.004846537628125382 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
      "Means: 0.8159199953079224, Stdev: 0.008527484826380416 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
      "Means: 0.8107466499010721, Stdev: 0.006761164276373468 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
      "Means: 0.813266654809316, Stdev: 0.007459052007729178 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
      "Means: 0.8207600116729736, Stdev: 0.002314704724269725 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 512}\n",
      "Means: 0.7575466632843018, Stdev: 0.003093088173462813 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
      "Means: 0.779093325138092, Stdev: 0.0021826843433676874 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
      "Means: 0.7863866686820984, Stdev: 0.003557449704999579 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
      "Means: 0.7901333371798197, Stdev: 0.004952719184902199 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
      "Means: 0.7924533486366272, Stdev: 0.0035890684483205304 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
      "Means: 0.7938400109608968, Stdev: 0.0020114361861848793 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
      "Means: 0.7974533438682556, Stdev: 0.002550037302249457 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
      "Means: 0.7965733210245768, Stdev: 0.006502217856117449 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
      "Means: 0.8004533449808756, Stdev: 0.001731069007536387 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
      "Means: 0.7985866467157999, Stdev: 0.006093168151556639 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
      "Means: 0.8029866615931193, Stdev: 0.0029301894391957533 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
      "Means: 0.7951866587003072, Stdev: 0.007071766160708477 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
      "Means: 0.7978933453559875, Stdev: 0.002815259839254896 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
      "Means: 0.8028533260027567, Stdev: 0.001131523903723823 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
      "Means: 0.8027466734250387, Stdev: 0.0016864428201192258 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n",
      "Means: 0.8018933335940043, Stdev: 0.003821658318705966 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 512}\n"
     ]
    }
   ],
   "source": [
    "# save start time \n",
    "start = time()\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=hyper_parameters, \n",
    "                    n_jobs=-2, \n",
    "                    verbose=1, \n",
    "                    cv=3)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# save end time \n",
    "end = time()\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.458556234836578"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total run time \n",
    "total_run_time_in_miniutes = (end - start)/60\n",
    "total_run_time_in_miniutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu', 'learning_rate': 0.001, 'units': 512}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4941 - accuracy: 0.8548\n"
     ]
    }
   ],
   "source": [
    "# because all other optimization approaches are reporting test set score\n",
    "# let's calculate the test set score in this case \n",
    "best_model = grid_result.best_estimator_\n",
    "test_acc = best_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8547999858856201"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Results\n",
    " \n",
    "Identify and write the the best performing hyperparamter combination and model score. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9577db883482c6cded3836e5cfbf5a74",
     "grade": true,
     "grade_id": "cell-eb06d682d2790f6e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
    "\n",
    "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Stretch Goals\n",
    "\n",
    "- Feel free to run whatever gridserach experiments on whatever models you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is your open playground - be free to explore as you wish "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_433_Tune_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Unit_4 (Python3)",
   "language": "python",
   "name": "unit_4p"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
